{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Core Spark Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## What comes with Apache Spark\n",
    "\n",
    "* Spark is a distributed computing framework\n",
    "* Core Spark contains all the basic functionality of Spark\n",
    "  * Contains RDD class and all action/transformation methods which operate on RDDs\n",
    "* Other modules in Spark sit on top of Core Spark\n",
    "  * __Spark SQL__ introduces a DataFrame class which acts as a schema-based RDD allowing for SQL syntax for working with data\n",
    "  * __Spark Streaming__ adds a streaming context object which can be used to do Spark operations on batches of streaming data\n",
    "  * __MLib__ adds a collection of common machine learning algorithms which may be used with RDDs\n",
    "  * __GraphX__ indtroduces node and edge classes and a set of common algorithms for analyzing graphs of data\n",
    "\n",
    "<img src=\"figs/spark-stack.png\">\n",
    "#### (spark.apache.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Let's play with some data!\n",
    "To start with, let's use some features of Core Spark\n",
    "\n",
    "***\n",
    "\n",
    "### Import Moby Dick\n",
    "This will be our first data set to use with Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobyDick_RDD = sc.textFile(\"./data/MobyDick.txt\")\n",
    "print(\"Number of lines in Moby Dick = %d\" % mobyDick_RDD.count())\n",
    "mobyDick_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map\n",
    "We have imported Moby Dick as a text file which creates an RDD of strings, each element being a single line in the text file. Let's create a new RDD which has each element being a word in the text.\n",
    "\n",
    "<img src=\"figs/flatMap_map.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_RDD = mobyDick_RDD.flatMap(lambda x: x.split(\" \"))\n",
    "print(\"Number of words in Moby Dick = %d\" % words_RDD.count())\n",
    "words_RDD.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Map\n",
    "Let's make sure that the data is _clean_ by removing all non-alphabetic characters and making all the words lowercase. To do this, let's create a transformation function maping a string to a lowercase string containing only a-z. We will then apply a map to the words_RDD using the transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "def word_transform(word):\n",
    "    clean_word = regex.sub('', word).lower()\n",
    "    return clean_word\n",
    "\n",
    "s1 = \"te4st8 w0ord!.\"\n",
    "print(s1 + \" -> \" + word_transform(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_words_RDD = words_RDD.map(lambda x: word_transform(x))\n",
    "print(\"Number of words in Moby Dick = %d\" % words_RDD.count())\n",
    "clean_words_RDD.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Filter\n",
    "It looks like we have made some progress on cleaning the data set, but we still have some '' chars left over from blank lines. Let's filter those out of our data set. Additionally, we have lots of common words that are pretty uninteresting (it, the, a, ...). These are stop words and we can also filter those out of our data set. Let's do that using an anonymous funciton. Filter functions need to return True if word should be in new dataset, or False if they should be filtered.\n",
    "\n",
    "<img src=\"figs/filter.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_reader(filename):\n",
    "    lines = [line.strip() for line in open(filename)]\n",
    "    lines = [x.lower() for x in lines]\n",
    "    return (lines)\n",
    "\n",
    "stop_words = word_reader(\"./data/stopwords.txt\")\n",
    "stop_words.append('')\n",
    "print(stop_words[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_filtered_words_RDD = clean_words_RDD.filter(lambda word: word not in stop_words)\n",
    "print(\"Number of words in Moby Dick = %d\" % clean_filtered_words_RDD.count())\n",
    "clean_filtered_words_RDD.take(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"We just filtered %d stop words from the text!\" % (clean_words_RDD.count() - clean_filtered_words_RDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Map reduces\n",
    "\n",
    "We now have a RDD which contains a nice list of clean words from Moby Dick. Let's see how common certain words are by mapping words to a key value pair (word, 1) and then reducing on word key and summing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count_RDD = clean_filtered_words_RDD.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "print(\"Number of unique clean words in Moby Dick = %d\" % word_count_RDD.count())\n",
    "word_count_RDD.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### SortBy\n",
    "There is an arbitrary sort method for RDDs which allows you to work with non-trivial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_count_RDD = word_count_RDD.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_word_count_RDD.take(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Plotting word frequency\n",
    "Awesome! We now know how frequenctly unque words occur in Moby Dick. Let's visualize this now using a word cloud. We should first collect the RDD from Spark back to the driver (this is essentially creating a local data structure from an RDD). We could return the sorted words as a list of tuples using _collect()_, or we could return as a python dict using _collectAsMap()_. We will use list method for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_count = sorted_word_count_RDD.collect()\n",
    "sorted_word_count[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import path, system\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except:\n",
    "    os.system(\"./data/install_word_cloud.sh\")\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "whale_mask = imread(\"./figs/whale.png\")\n",
    "wc = WordCloud(background_color=\"black\", max_words=2000,\n",
    "               font_path='./figs/Verdana.ttf', mask=whale_mask,\n",
    "               width=1600, height=800)\n",
    "wc.generate_from_frequencies(sorted_word_count)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(\"./figs/whale_text.png\")\n",
    "\n",
    "# show\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Summary\n",
    "We have accomplished the following during this session:\n",
    "* Imported a txt file as a RDD where each element is a line int he txt file\n",
    "* Did a flat map to get a RDD where each element is a word in the txt file\n",
    "* Did a map transformation to make all words lowercase and stripping non-alphabetic characters\n",
    "* Did a map-reduce to make (key, value) elements of the RDD where key is the word in the txt file and value is the number of times that word occured in the txt file\n",
    "* Sorted the (key, value) RDD using custom sortBy function\n",
    "* Collected the (key, value) RDD as a list of tuples to the python driver\n",
    "* Plotted a word cloud of using the list of (key, value) RDDs\n",
    "\n",
    "Many of these steps can be performed at once by chaining actions/transformations together. For example, we can directly obtain the final (key, value) list as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_count_easy = sc.textFile(\"./data/MobyDick.txt\").\\\n",
    "                            flatMap(lambda x: x.split(\" \")).\\\n",
    "                            map(lambda x: word_transform(x)).\\\n",
    "                            filter(lambda word: word not in stop_words).\\\n",
    "                            map(lambda x: (x,1)).\\\n",
    "                            reduceByKey(lambda x,y: x+y).\\\n",
    "                            sortBy(lambda x: x[1], ascending=False).\\\n",
    "                            collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_count_easy[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_word_count[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## More word clouds for entire bookes\n",
    "***\n",
    "### Function to take in text file and return plot of word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_plotter(text_file, mask_file=\"\"):\n",
    "    %matplotlib inline\n",
    "    from os import path, system\n",
    "    from scipy.misc import imread\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    try:\n",
    "        from wordcloud import WordCloud\n",
    "    except:\n",
    "        os.system(\"./data/install_word_cloud.sh\")\n",
    "        from wordcloud import WordCloud\n",
    "\n",
    "    if mask_file != \"\":\n",
    "        whale_mask = imread(mask_file)\n",
    "        wc = WordCloud(background_color=\"black\", max_words=2000,\n",
    "                       font_path='./figs/Verdana.ttf', mask=whale_mask,\n",
    "                       width=1600, height=800)\n",
    "    else:         \n",
    "        wc = WordCloud(background_color=\"black\", max_words=2000,\n",
    "                       font_path='./figs/Verdana.ttf',\n",
    "                       width=1600, height=800)\n",
    "   \n",
    "    dat = sc.textFile(text_file).\\\n",
    "                      flatMap(lambda x: x.split(\" \")).\\\n",
    "                      map(lambda x: word_transform(x)).\\\n",
    "                      filter(lambda word: word not in stop_words).\\\n",
    "                      map(lambda x: (x,1)).\\\n",
    "                      reduceByKey(lambda x,y: x+y).\\\n",
    "                      sortBy(lambda x: x[1], ascending=False).\\\n",
    "                      collect()\n",
    "    wc.generate_from_frequencies(dat)\n",
    "\n",
    "    # store to file\n",
    "    wc.to_file(\"./figs/\" + text_file.split(\"/\")[-1].split(\".txt\")[0] + \"_wordCloud.png\")\n",
    "\n",
    "    # show\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(16, 9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sleepy Hollow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_plotter(\"./data/SleepyHollow.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Treasure Island"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_plotter(\"./data/TreasureIsland.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The Republic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_plotter(\"./data/Republic.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Additional Exercises With Spark\n",
    "## Using Scrabble scores to analyze book complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Letter point values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LP = {'a':1,'b':4,'c':4,'d':2,'e':1,'f':4,'g':3,\\\n",
    "      'h':3,'i':1,'j':10,'k':5,'l':2,'m':4,'n':2,\\\n",
    "      'o':1,'p':4,'q':10,'r':1,'s':1,'t':1,'u':2,\\\n",
    "      'v':5,'w':4,'x':8,'y':3,'z':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Function which takes a string (word) and returns total point value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word (string)\n",
    "# returns number of points in word (int)\n",
    "def word_points(word):\n",
    "    points = 0\n",
    "    for letter in word:\n",
    "        try:\n",
    "            points += LP[letter.lower()]\n",
    "        except:\n",
    "            print(\"%s not in LP\" % letter)\n",
    "            points += 0\n",
    "    return points\n",
    "\n",
    "# should return 4\n",
    "word_points(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Function which takes a txt filename and returns average scrabble score (double)\n",
    "#### Average scrabble score defined as [total score for all words in book] / [total number of words in book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "def average_scrabble_score(book_title, text_file):\n",
    "    rd0 = sc.textFile(text_file).\\\n",
    "                      flatMap(lambda x: x.split(\" \")).\\\n",
    "                      map(lambda x: word_transform(x)).\\\n",
    "                      filter(lambda word: word not in stop_words).\\\n",
    "                      map(lambda x: word_points(x))\n",
    "\n",
    "    rd0.cache()\n",
    "    N_words = rd0.count()\n",
    "    N_points = rd0.reduce(lambda x,y: x+y)\n",
    "    Ave_Score = float(N_points)/float(N_words)\n",
    "    all_points = rd0.collect()\n",
    "    n, bins, patches = plt.hist(all_points, 50, normed=1, facecolor='green', alpha=0.5)\n",
    "    (mu, sigma) = norm.fit(all_points)\n",
    "    y = mlab.normpdf(bins, mu, sigma)\n",
    "    l = plt.plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "    plt.xlabel('Word point value', fontsize=16)\n",
    "    plt.ylabel('Normalized Frequency', fontsize=16)\n",
    "    plt.title(\"%s Average score %2.2f\" % (book_title, Ave_Score))\n",
    "    plt.show()\n",
    "    \n",
    "    return (bins, y)\n",
    "\n",
    "ret = average_scrabble_score(\"Moby Dick\", \"./data/MobyDick.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Run the function on a collection of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Books = {\"Moby Dick\":                     {\"fid\": \"./data/MobyDick.txt\",       \"year\": 1851},\n",
    "         \"Treasure Island\":               {\"fid\": \"./data/TreasureIsland.txt\", \"year\": 1883},\n",
    "         \"The Republic\":                  {\"fid\": \"./data/Republic.txt\",       \"year\": -380},\n",
    "         \"Legend of Sleepy Hollow\":       {\"fid\": \"./data/SleepyHollow.txt\",   \"year\": 1820},\n",
    "         \"Pride and Predjudice\":          {\"fid\": \"./data/PridePrejudice.txt\", \"year\": 1813},\n",
    "         \"War and Peace\":                 {\"fid\": \"./data/WarAndPeace.txt\",    \"year\": 1869},\n",
    "         \"Julius Ceasar\":                 {\"fid\": \"./data/JuliusCaesar.txt\",   \"year\": 1599},\n",
    "         }\n",
    "\n",
    "for book in Books.keys():\n",
    "    try:\n",
    "        Books[book][\"score\"] = average_scrabble_score(book, Books[book][\"fid\"])\n",
    "    except:\n",
    "        print(\"%s failed!!\" % book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
