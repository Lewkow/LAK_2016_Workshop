{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "## Introduction to Resiliant Distributed Datasets (RDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "* Spark is a distributed computing framework for big data. \n",
    "* Clusters of worker machines operate on Resiliant Distributed Datasets (RDDs) in paralell. \n",
    "* Spark is optimized to utilized worker memory as much as possible, allowing for fast computation on large datasets.\n",
    "\n",
    "<img src=\"figs/RDD_1.png\">\n",
    "#### A spark driver, either through an application or shell, connects to a cluster consisting of worker machines. (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* RDDs are collections of data either from a central source which is then parallelized (file on local machine) or from a distributed data store (HDFS, Cassandra etc).\n",
    "* RDDs can consist of common data types and arbitrary objects.\n",
    "* An RDD is split between several partitions and distributed to the different worker machines.\n",
    "<img src=\"figs/RDD_2.png\">\n",
    "#### Large dataset partitioned between different worker machines in a Spark cluster (Databricks Workshop 2015) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Example RDD consisting of logging data\n",
    "\n",
    "<img src=\"figs/RDD_3.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Creating an RDD from a collection of data on the driver and distributing them to the worker machines\n",
    "\n",
    "<img src=\"figs/RDD_4.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create RDD from a python list\n",
    "wordsRDD = sc.parallelize([\"fish\", \"cats\", \"dogs\"])\n",
    "\n",
    "# View object type\n",
    "print(wordsRDD)\n",
    "\n",
    "# Get number of objects in RDD\n",
    "print(wordsRDD.count())\n",
    "\n",
    "# Get the first object in the RDD\n",
    "print(wordsRDD.first())\n",
    "\n",
    "# Collect all the objects in the RDD from Spark back to driver (this notebook)\n",
    "print(wordsRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Creating an RDD from a file\n",
    "  * Can come from local machine (small files)\n",
    "  * Can read files directly from HDFS and Cassandra\n",
    "    * example: > newRDD = sc.textFile(\"hdfs://localhost:9000/user/data/file.txt\")\n",
    "\n",
    "<img src=\"figs/RDD_5.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in Moby Dick text file (reads in one line per RDD entry)\n",
    "mobyDickRDD = sc.textFile(\"./data/MobyDick.txt\")\n",
    "\n",
    "# Count how many lines are in RDD\n",
    "print(\"There is a total of %d lines in Moby Dick\" % mobyDickRDD.count())\n",
    "\n",
    "# Take first 5 lines from RDD\n",
    "mobyDickRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Typical workflow\n",
    "  * Data is read from external sources (HDFS, Cassandra, local file...) into an RDD\n",
    "  * Transformations are performed on RDD to create new RDDs\n",
    "    * In this example, the main RDD is filtered to create an RDD which only contains Error messages\n",
    "\n",
    "<img src=\"figs/RDD_6.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Filtered RDDs can be coalesced to reduce the number of partitions which make up that RDD\n",
    "  * Used when filtered RDDs become sparse compared to their parent RDD\n",
    "\n",
    "<img src=\"figs/RDD_7.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The number of paritions for a RDD can be specified when RDD is created\n",
    "# This is automatically done if no parameter is passed\n",
    "N_partitions = 4\n",
    "team = sc.parallelize([\"Al\", \"Ani\", \"Jackie\", \"Lalitha\", \"Mark\", \"Neil\", \"Nick\", \"Shirin\"], N_partitions)\n",
    "print(team.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "* Standard workflow for a Spark application\n",
    "\n",
    "<img src=\"figs/RDD_8.png\">\n",
    "#### (Databricks Workshop 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
